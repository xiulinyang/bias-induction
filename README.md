# human-like-bias

# Generalization
| **Paper**                                                    | **Venue** | **Year** | **Code**                                               | 
| ------------------------------------------------------------ | --------- | -------- | ------------------------------------------------------------ |
| [Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations](https://aclanthology.org/2024.emnlp-main.645/) | EMNLP | 2024| [code](https://github.com/namednil/step)|
|[SIP: Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation](https://aclanthology.org/2024.acl-long.355/) | ACL| 2024 | [code](https://github.com/namednil/sip)|
|[Injecting structural hints: Using language models to study inductive biases in language learning](https://aclanthology.org/2023.findings-emnlp.563/) | EMNLP findings | 2023| [code](https://github.com/toizzy/injecting-structural-hints)|
|[Does Vision Accelerate Hierarchical Generalization in Neural Language Learners?](https://aclanthology.org/2025.coling-main.127/)|COLING| 2025|NA|
|[Distilling symbolic priors for concept learning into neural networks](https://arxiv.org/abs/2402.07035)| Preprint | 2024| NA|
|[Meta-learning as a bridge between neural networks and symbolic Bayesian models.](https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/metalearning-as-a-bridge-between-neural-networks-and-symbolic-bayesian-models/185DB00366FD4F9B218E36F32886242F)|Behavioral and Brain Sciences| 2024 |[code](https://github.com/marcelbinz/meta-learned-models)|
|[Human-like systematic generalization through a meta-learning neural network](https://www.nature.com/articles/s41586-023-06668-3)|Nature|2023|NA|

# Psychometric
| **Paper**                                                    | **Venue** | **Year** | **Code**                                               | 
| ------------------------------------------------------------ | --------- | -------- | ------------------------------------------------------------ |
[Language Models Grow Less Humanlike beyond Phase Transition](https://arxiv.org/abs/2502.18802)|Preprint| 2025| NA|
|[Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs](https://arxiv.org/abs/2309.07311)|ICLR|2024| NA|
# Training


# Inference
